---
title: "NN"
author: "Paula"
date: "2 de diciembre de 2018"
output: html_document
---


#REDES NEURONALES

Deep networks construyen representaciones de patterns in los datos

capas consecutivas contruyen una representación sofisticada de los raw datas


#Long Short-Term Memory Network

Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.

LSTMs are explicitly designed to avoid the long-term dependency problem.


##batch size

__The batch size limits the number of samples to be shown to the network before a weight update can be performed.__

__Mi red actualiza sus pesos después de cada batch__


##Online Learning (Batch Size = 1)

This is where the batch size is set to a value of 1 and the network weights are updated after each training example.

This can have the effect of faster learning, but also adds instability to the learning process as the weights widely vary with each batch.

Nevertheless, this will allow us to make one-step forecasts on the problem. The only change required is setting n_batch to 1.


##Batch Forecasting (Batch Size = N)


Another solution is to make all predictions at once in a batch.

This would mean that we could be very limited in the way the model is used.

We would have to use all predictions made at once, or only keep the first prediction and discard the rest.


##Copy Weights

A better solution is to use different batch sizes for training and predicting.

The way to do this is to copy the weights from the fit network and to create a new network with the pre-trained weights.

We can do this easily enough using the get_weights() and set_weights() functions in the Keras API,
This creates a new model that is compiled with a batch size of 1.

ejemplo, interesante

```{py}
# design network
model = Sequential()
model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
# fit network
for i in range(n_epoch):
	model.fit(X, y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)
	model.reset_states()
# re-define the batch size
n_batch = 1
# re-define model
new_model = Sequential()
new_model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))
new_model.add(Dense(1))
# copy weights
old_weights = model.get_weights()
new_model.set_weights(old_weights)
# compile model
new_model.compile(loss='mean_squared_error', optimizer='adam')

```


# Epochs:
One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.
Since, one epoch is too big to feed to the computer at once we divide it in several smaller batches. 
durante cada epochs yo sigo optimizando mis pesos. que es lo que hago en fit..


As the number of epochs increases, more number of times the weight are changed in the neural network and the curve goes from underfitting to optimal to overfitting curve.


#Batch Size

you divide dataset into Number of Batches or sets or parts.

In “batch” learning the whole training set is used to compute the current model performance in the loss function. This can be infeasible for large data sets so “mini-batch” may be used instead, computing the loss function over a chunk or subset of the data. 

#iteration

Iterations is the number of batches needed to complete one epoch.


#rolling-forecast scenario or walk-forward model validation

Each time step of the test dataset will be walked one at a time.
uso mi dato de test X_{t-1} para obtener yhat_t.

This mimics a real-world scenario where new Shampoo Sales observations would be available each month and used in the forecasting of the following month


# LSTM state

